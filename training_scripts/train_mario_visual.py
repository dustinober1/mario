#!/usr/bin/env python3
"""
Mario RL Training with Visual Rendering and Fixed Slice Error

This version shows the game visually while training and fixes the buffer slicing error.
"""

import sys
import time
import os
from pathlib import Path

print("ğŸš€ Mario RL Training - Visual GPU Optimized")
print("=" * 50)

print("1. Loading libraries...")
import numpy as np
import torch
import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import BaseCallback
from gym_super_mario_bros.actions import SIMPLE_MOVEMENT
from gym_super_mario_bros import make
from nes_py.wrappers import JoypadSpace

print("   âœ“ Libraries loaded")

def detect_best_device():
    """Detect and return the best available computing device."""
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        device_name = "Apple Silicon GPU (MPS)"
        print(f"   ğŸ Using Metal Performance Shaders: {device_name}")
    elif torch.cuda.is_available():
        device = torch.device("cuda")
        device_name = f"NVIDIA GPU: {torch.cuda.get_device_name()}"
        print(f"   ğŸ”¥ Using CUDA GPU: {device_name}")
    else:
        device = torch.device("cpu")
        device_name = "CPU"
        print(f"   ğŸ’» Using CPU: {device_name}")
    
    return device, device_name

class GymToGymnasiumWrapper(gym.Env):
    """Wrapper to make old gym environments compatible with gymnasium."""
    
    def __init__(self, env):
        super().__init__()
        self._env = env
        # Convert old gym spaces to gymnasium spaces
        if hasattr(env.observation_space, 'shape'):
            self.observation_space = gym.spaces.Box(
                low=env.observation_space.low,
                high=env.observation_space.high,
                shape=env.observation_space.shape,
                dtype=env.observation_space.dtype
            )
        else:
            self.observation_space = env.observation_space
            
        # Convert action space
        if hasattr(env.action_space, 'n'):
            self.action_space = gym.spaces.Discrete(env.action_space.n)
        else:
            self.action_space = env.action_space
        
    def reset(self, **kwargs):
        obs = self._env.reset()
        return obs, {}
    
    def step(self, action):
        obs, reward, done, info = self._env.step(action)
        return obs, reward, done, False, info
    
    def render(self):
        return self._env.render()
    
    def close(self):
        return self._env.close()

class VisualPerformanceCallback(BaseCallback):
    """Enhanced callback with visual rendering and fixed buffer handling."""
    
    def __init__(self, verbose=1, device_name="Unknown", render_freq=100):
        super().__init__(verbose)
        self.device_name = device_name
        self.best_mean_reward = float('-inf')
        self.start_time = time.time()
        self.render_freq = render_freq
        self.step_count = 0
        
    def _on_step(self) -> bool:
        self.step_count += 1
        
        # Render the game periodically
        if self.step_count % self.render_freq == 0:
            try:
                # Try to render the environment
                if hasattr(self.training_env, 'render'):
                    self.training_env.render()
                elif hasattr(self.training_env, 'envs') and len(self.training_env.envs) > 0:
                    # For vectorized environments, render the first environment
                    first_env = self.training_env.envs[0]
                    if hasattr(first_env, 'render'):
                        first_env.render()
                    elif hasattr(first_env, 'env') and hasattr(first_env.env, 'render'):
                        first_env.env.render()
                    elif hasattr(first_env, '_env') and hasattr(first_env._env, 'render'):
                        first_env._env.render()
            except Exception as e:
                # Silently handle render errors
                if self.step_count % 1000 == 0:  # Only log occasionally
                    print(f"   ğŸ® Rendering note: {type(e).__name__}")
        
        return True
    
    def _on_rollout_end(self) -> None:
        # Fixed buffer handling - check if ep_info_buffer exists and has episodes
        if hasattr(self.model, 'ep_info_buffer') and self.model.ep_info_buffer:
            try:
                # Convert to list if it's not already and get recent episodes
                if hasattr(self.model.ep_info_buffer, '__iter__'):
                    episode_list = list(self.model.ep_info_buffer)
                    if len(episode_list) > 0:
                        # Get last 10 episodes or all if fewer than 10
                        recent_episodes = episode_list[-min(10, len(episode_list)):]
                        
                        if recent_episodes:
                            mean_reward = np.mean([ep["r"] for ep in recent_episodes if "r" in ep])
                            mean_length = np.mean([ep["l"] for ep in recent_episodes if "l" in ep])
                            
                            if mean_reward > self.best_mean_reward:
                                self.best_mean_reward = mean_reward
                                print(f"\n   ğŸ‰ NEW BEST! Mean reward: {mean_reward:.1f}")
                            
                            # Calculate training speed
                            elapsed = time.time() - self.start_time
                            steps_per_second = self.num_timesteps / elapsed if elapsed > 0 else 0
                            
                            if self.num_timesteps % 4000 == 0:
                                print(f"\n   ğŸ“Š Progress: {self.num_timesteps:,} steps")
                                print(f"      Mean reward: {mean_reward:.1f}")
                                print(f"      Mean episode length: {mean_length:.1f}")
                                print(f"      Training speed: {steps_per_second:.1f} steps/sec on {self.device_name}")
                                print(f"      ğŸ® Visual rendering every {self.render_freq} steps")
            except Exception as e:
                # Handle any buffer access errors gracefully
                print(f"   âš ï¸  Buffer access issue (normal during early training): {type(e).__name__}")

def create_visual_mario_env():
    """Create a Mario environment with visual rendering enabled."""
    # Create base environment - old gym style doesn't use render_mode parameter
    env = make('SuperMarioBros-1-1-v0')
    env = JoypadSpace(env, SIMPLE_MOVEMENT)
    env = GymToGymnasiumWrapper(env)
    return env

def create_headless_mario_env():
    """Create a Mario environment without rendering for faster training."""
    env = make('SuperMarioBros-1-1-v0')
    env = JoypadSpace(env, SIMPLE_MOVEMENT)
    env = GymToGymnasiumWrapper(env)
    return env

def main():
    """Run GPU-optimized Mario training with visual feedback."""
    try:
        # Detect best device
        device, device_name = detect_best_device()
        
        # Set environment variables for optimization
        if device.type == "mps":
            os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
        
        print("\n2. Creating visual Mario environment...")
        
        # Ask user preference for rendering
        print("   ğŸ® Visual rendering options:")
        print("      - Visual training shows Mario gameplay during training")
        print("      - Headless training runs faster without visuals")
        
        # For this demo, let's use visual mode
        use_visual = True
        
        if use_visual:
            env = DummyVecEnv([lambda: Monitor(create_visual_mario_env())])
            print("   âœ“ Created VISUAL Mario environment (you'll see Mario playing!)")
        else:
            env = DummyVecEnv([lambda: Monitor(create_headless_mario_env())])
            print("   âœ“ Created headless Mario environment (faster training)")
        
        print(f"   âœ“ Using GPU acceleration: {device_name}")
        
        print("\n3. Creating GPU-optimized AI agent...")
        
        # GPU-optimized hyperparameters
        timesteps = 50000 if use_visual else 100000  # Fewer steps for visual demo
        
        model = PPO(
            'CnnPolicy',
            env,
            verbose=0,
            device=device,
            
            # GPU-optimized hyperparameters
            learning_rate=3e-4,
            n_steps=1024 if use_visual else 2048,  # Smaller steps for visual feedback
            batch_size=128 if use_visual else 256,  # Smaller batches for visual demo
            n_epochs=10,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            ent_coef=0.01,
            vf_coef=0.5,
            max_grad_norm=0.5,
        )
        
        print(f"   âœ“ PPO agent created for {device_name}")
        print(f"   âœ“ Batch size: {128 if use_visual else 256}")
        print(f"   âœ“ Steps per update: {1024 if use_visual else 2048}")
        
        print(f"\nğŸš€ Starting GPU-optimized training with visuals!")
        print(f"   ğŸ¯ Target: {timesteps:,} timesteps")
        print(f"   ğŸ”¥ Device: {device_name}")
        print(f"   ğŸ® Visual rendering: {'ENABLED' if use_visual else 'DISABLED'}")
        print("-" * 50)
        
        # Create visual performance callback with rendering
        callback = VisualPerformanceCallback(
            device_name=device_name, 
            render_freq=50 if use_visual else 1000  # Render every 50 steps if visual
        )
        
        # Training with GPU acceleration and visual feedback
        start_time = time.time()
        
        model.learn(
            total_timesteps=timesteps,
            callback=callback,
            progress_bar=True
        )
        
        training_time = time.time() - start_time
        steps_per_second = timesteps / training_time
        
        print(f"\nâœ… GPU Visual Training completed!")
        print(f"   â±ï¸  Time: {training_time:.1f} seconds")
        print(f"   ğŸš€ Speed: {steps_per_second:.1f} steps/second")
        
        # Save model
        model_name = "mario_visual_gpu" if use_visual else "mario_gpu_optimized"
        model.save(model_name)
        print(f"ğŸ’¾ Model saved as '{model_name}'")
        
        # Test the trained model with visuals
        print(f"\nğŸ† Testing GPU-trained model with visuals:")
        print("Running 3 test episodes...")
        
        # Create visual environment for testing
        test_env = DummyVecEnv([lambda: Monitor(create_visual_mario_env())])
        
        test_results = []
        
        for episode in range(3):
            print(f"\n   ğŸ® Episode {episode + 1} - Watch Mario play!")
            obs = test_env.reset()
            episode_reward = 0
            max_x = 0
            step_count = 0
            
            for _ in range(1000):  # Shorter episodes for demo
                action, _ = model.predict(obs, deterministic=True)
                obs, reward, done, info = test_env.step(action)
                episode_reward += np.sum(reward) if hasattr(reward, '__len__') else reward
                step_count += 1
                
                # Track best x position
                if info and len(info) > 0:
                    for env_info in info:
                        if env_info and 'x_pos' in env_info:
                            max_x = max(max_x, env_info['x_pos'])
                
                # Add small delay for visual enjoyment
                if use_visual:
                    time.sleep(0.02)  # 50 FPS visual
                
                if np.any(done):
                    break
            
            test_results.append({
                'distance': max_x,
                'reward': episode_reward,
                'steps': step_count
            })
            
            print(f"      Distance: {max_x:,} pixels, Reward: {episode_reward:.1f}, Steps: {step_count}")
        
        # Final results
        avg_distance = np.mean([r['distance'] for r in test_results])
        best_distance = max([r['distance'] for r in test_results])
        avg_reward = np.mean([r['reward'] for r in test_results])
        
        print(f"\nğŸ“Š GPU Visual Training Results:")
        print(f"   ğŸƒ Average distance: {avg_distance:,.1f} pixels")
        print(f"   ğŸ¯ Best distance: {best_distance:,} pixels")
        print(f"   ğŸ’° Average reward: {avg_reward:.1f}")
        print(f"   ğŸ”¥ Device used: {device_name}")
        print(f"   âš¡ Training speed: {steps_per_second:.1f} steps/sec")
        
        print(f"\nğŸ’¡ Visual Training Benefits:")
        print(f"   ğŸ® Watched Mario learn to play in real-time!")
        print(f"   ğŸš€ GPU acceleration with Metal Performance Shaders")
        print(f"   ğŸ”§ Fixed buffer slicing error")
        print(f"   ğŸ‘€ Visual feedback during training and testing")
        
        if best_distance > 2000:
            print("   ğŸ† AMAZING! Mario learned to navigate well!")
        elif best_distance > 1000:
            print("   ğŸŒŸ GREAT! Mario is making good progress!")
        elif best_distance > 500:
            print("   ğŸ‘ GOOD! Mario is learning the basics!")
        else:
            print("   ğŸ“ˆ PROGRESS! Mario is starting to learn!")
        
        test_env.close()
        
    except KeyboardInterrupt:
        print("\nâ¹ Training interrupted by user")
        if 'model' in locals():
            model.save("mario_visual_interrupted")
            print("ğŸ’¾ Progress saved as 'mario_visual_interrupted'")
    except Exception as e:
        print(f"\nâŒ Error occurred: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if 'env' in locals():
            env.close()
        print("ğŸ Visual GPU training session complete!")

if __name__ == "__main__":
    main()